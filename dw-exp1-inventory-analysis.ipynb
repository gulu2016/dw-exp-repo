{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31113b4a-47b7-4473-b89a-57affba92a7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, DateType\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProductTableLoader\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49bba8da-432e-4a4d-a368-640043ecf2fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 定义产品表的表结构\n",
    "product_schema = StructType([\n",
    "    StructField(\"Product ID\", StringType(), True),\n",
    "    StructField(\"Product Name\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"Brand\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Cost Price\", FloatType(), True),\n",
    "    StructField(\"Selling Price\", FloatType(), True),\n",
    "    StructField(\"Supplier ID\", StringType(), True),\n",
    "    StructField(\"Stock Keeping Unit (SKU)\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f55a0a4-d9fb-4049-8d90-f321d39cae39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+------+------------+----------+-------------+-----------+------------------------+\n|Product ID|Product Name| Category| Brand| Description|Cost Price|Selling Price|Supplier ID|Stock Keeping Unit (SKU)|\n+----------+------------+---------+------+------------+----------+-------------+-----------+------------------------+\n|        P1|    Product1|Category1|Brand1|Description1|      10.5|         20.5|  Supplier1|                    SKU1|\n|        P2|    Product2|Category2|Brand2|Description2|      15.0|         25.0|  Supplier2|                    SKU2|\n|        P3|    Product3|Category3|Brand3|Description3|      12.0|         22.0|  Supplier3|                    SKU3|\n+----------+------------+---------+------+------------+----------+-------------+-----------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 从字符串中读取产品记录并加载到 DataFrame 中\n",
    "product_records = [\n",
    "    (\"P1\",\"Product1\",\"Category1\",\"Brand1\",\"Description1\",10.5,20.5,\"Supplier1\",\"SKU1\"),\n",
    "    (\"P2\",\"Product2\",\"Category2\",\"Brand2\",\"Description2\",15.0,25.0,\"Supplier2\",\"SKU2\"),\n",
    "    (\"P3\",\"Product3\",\"Category3\",\"Brand3\",\"Description3\",12.0,22.0,\"Supplier3\",\"SKU3\")\n",
    "]\n",
    "\n",
    "product_df = spark.createDataFrame(product_records, schema=product_schema)\n",
    "\n",
    "# 加载到 ODS 表中\n",
    "# 假设 ODS 表已存在或需要创建\n",
    "ods_table_name = \"exp1_ods_product_table\"\n",
    "\n",
    "# 将时间 DataFrame 保存到 ODS 表中\n",
    "product_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(ods_table_name)\n",
    "\n",
    "# 从 ODS 表中读取数据\n",
    "product_ods_df = spark.table(ods_table_name)\n",
    "\n",
    "# 展示 DataFrame\n",
    "product_ods_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a741acbf-ca22-479a-b8fa-358f9b079e29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---------+------+------------+----------+-------------+-----------+------------------------+\n|Product ID|Product Name| Category| Brand| Description|Cost Price|Selling Price|Supplier ID|Stock Keeping Unit (SKU)|\n+----------+------------+---------+------+------------+----------+-------------+-----------+------------------------+\n|        P1|    Product1|Category1|Brand1|Description1|      10.5|         20.5|  Supplier1|                    SKU1|\n|        P2|    Product2|Category2|Brand2|Description2|      15.0|         25.0|  Supplier2|                    SKU2|\n|        P3|    Product3|Category3|Brand3|Description3|      12.0|         22.0|  Supplier3|                    SKU3|\n+----------+------------+---------+------+------------+----------+-------------+-----------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# product的dimension表\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Product Dimension Table Creation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "product_ods_df = spark.table(ods_table_name)\n",
    "\n",
    "# 创建维度表\n",
    "dimension_df = product_ods_df.select(\"Product ID\", \"Product Name\", \"Category\", \"Brand\", \"Description\", \"Cost Price\", \"Selling Price\", \"Supplier ID\",\"Stock Keeping Unit (SKU)\")\n",
    "\n",
    "# 加载到 DIM 表中\n",
    "# 假设 DIM 表已存在或需要创建\n",
    "dim_table_name = \"exp1_dim_product_table\"\n",
    "\n",
    "# 将时间 DataFrame 保存到 DIM 表中\n",
    "dimension_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(dim_table_name)\n",
    "\n",
    "dimension_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5682492-68ca-42fb-a01a-af88bf7e819c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+----------+\n|Product ID|Warehouse ID|Quantity|Stock Date|\n+----------+------------+--------+----------+\n|        P2|          W2|     150|2023-01-02|\n|        P1|          W1|     100|2023-01-01|\n|        P3|          W1|     200|2023-01-03|\n+----------+------------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Define the inventory table schema\n",
    "inventory_schema = StructType([\n",
    "    StructField(\"Product ID\", StringType(), True),\n",
    "    StructField(\"Warehouse ID\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Stock Date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Construct the test data\n",
    "warehouse_data = [\n",
    "    (\"P1\", \"W1\", 100, \"2023-01-01\"),\n",
    "    (\"P2\", \"W2\", 150, \"2023-01-02\"),\n",
    "    (\"P3\", \"W1\", 200, \"2023-01-03\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "inventory_df = spark.createDataFrame(warehouse_data, schema=inventory_schema)\n",
    "\n",
    "# Apply the to_date() transformation to the \"Stock Date\" column\n",
    "inventory_df = inventory_df.withColumn(\"Stock Date\", to_date(inventory_df[\"Stock Date\"]))\n",
    "\n",
    "# 加载到 OD 表中\n",
    "# 假设 OD 表已存在或需要创建\n",
    "ods_table_name = \"exp1_ods_inventory_table\"\n",
    "\n",
    "# 将时间 DataFrame 保存到 OD 表中\n",
    "inventory_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(ods_table_name)\n",
    "\n",
    "# 从 OD 表中读取数据\n",
    "inventory_ods_df = spark.table(ods_table_name)\n",
    "\n",
    "# 展示 DataFrame\n",
    "inventory_ods_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6a550f-b53c-48ed-9386-fbecdf2a0300",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock Sales Data:\n+--------+----------+-----------+--------+----------+----------+-----------+--------------+\n|Order ID|Product ID|Customer ID|Quantity|Sales Date|Unit Price|Total Price|Payment Method|\n+--------+----------+-----------+--------+----------+----------+-----------+--------------+\n|  ORD001|        P1|    CUST001|       2|2023-01-01|      10.0|       20.0|          Cash|\n|  ORD002|        P2|    CUST002|       3|2023-01-02|      15.0|       45.0|   Credit Card|\n|  ORD003|        P3|    CUST003|       1|2023-01-03|      20.0|       20.0|Online Payment|\n+--------+----------+-----------+--------+----------+----------+-----------+--------------+\n\n+--------+----------+-----------+--------+----------+----------+-----------+--------------+\n|Order ID|Product ID|Customer ID|Quantity|Sales Date|Unit Price|Total Price|Payment Method|\n+--------+----------+-----------+--------+----------+----------+-----------+--------------+\n|  ORD003|        P3|    CUST003|       1|2023-01-03|      20.0|       20.0|Online Payment|\n|  ORD002|        P2|    CUST002|       3|2023-01-02|      15.0|       45.0|   Credit Card|\n|  ORD001|        P1|    CUST001|       2|2023-01-01|      10.0|       20.0|          Cash|\n+--------+----------+-----------+--------+----------+----------+-----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from pyspark.sql.functions import to_date\n",
    "from datetime import datetime\n",
    "\n",
    "# Define sales table structure\n",
    "sales_schema = StructType([\n",
    "    StructField(\"Order ID\", StringType(), True),\n",
    "    StructField(\"Product ID\", StringType(), True),\n",
    "    StructField(\"Customer ID\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Sales Date\", DateType(), True),\n",
    "    StructField(\"Unit Price\", DoubleType(), True),\n",
    "    StructField(\"Total Price\", DoubleType(), True),\n",
    "    StructField(\"Payment Method\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Construct test data with updated date format\n",
    "sales_data = [\n",
    "    (\"ORD001\", \"P1\", \"CUST001\", 2, datetime.strptime(\"2023-01-01\", \"%Y-%m-%d\"), 10.0, 20.0, \"Cash\"),\n",
    "    (\"ORD002\", \"P2\", \"CUST002\", 3, datetime.strptime(\"2023-01-02\", \"%Y-%m-%d\"), 15.0, 45.0, \"Credit Card\"),\n",
    "    (\"ORD003\", \"P3\", \"CUST003\", 1, datetime.strptime(\"2023-01-03\", \"%Y-%m-%d\"), 20.0, 20.0, \"Online Payment\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "sales_df = spark.createDataFrame(sales_data, schema=sales_schema)\n",
    "\n",
    "# Show DataFrame\n",
    "print(\"Mock Sales Data:\")\n",
    "sales_df.show()\n",
    "\n",
    "# 加载到 OD 表中\n",
    "# 假设 OD 表已存在或需要创建\n",
    "ods_table_name = \"exp1_ods_sales_table\"\n",
    "\n",
    "# 将时间 DataFrame 保存到 OD 表中\n",
    "sales_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(ods_table_name)\n",
    "\n",
    "# 从 OD 表中读取数据\n",
    "sales_ods_df = spark.table(ods_table_name)\n",
    "\n",
    "# 展示 DataFrame\n",
    "sales_ods_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d5db3df-1d72-47a9-9f9f-f4b5a6256653",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+-------+----+\n|      Date|      Day|  Month|Quarter|Year|\n+----------+---------+-------+-------+----+\n|2023-01-03|Wednesday|January|     Q1|2023|\n|2023-01-02|  Tuesday|January|     Q1|2023|\n|2023-01-01|   Monday|January|     Q1|2023|\n+----------+---------+-------+-------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# 定义时间表的表结构\n",
    "time_schema = StructType([\n",
    "    StructField(\"Date\", StringType(), nullable=False),\n",
    "    StructField(\"Day\", StringType(), nullable=False),\n",
    "    StructField(\"Month\", StringType(), nullable=False),\n",
    "    StructField(\"Quarter\", StringType(), nullable=False),\n",
    "    StructField(\"Year\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# 构造测试数据\n",
    "test_data = [\n",
    "    (\"2023-01-01\", \"Monday\", \"January\", \"Q1\", 2023),\n",
    "    (\"2023-01-02\", \"Tuesday\", \"January\", \"Q1\", 2023),\n",
    "    (\"2023-01-03\", \"Wednesday\", \"January\", \"Q1\", 2023),\n",
    "    # 添加更多测试数据...\n",
    "]\n",
    "\n",
    "# 创建 DataFrame\n",
    "time_df = spark.createDataFrame(test_data, schema=time_schema)\n",
    "\n",
    "# 加载到 ODS 表中\n",
    "# 假设 ODS 表已存在或需要创建\n",
    "ods_table_name = \"exp1_ods_time_table\"\n",
    "\n",
    "# 将时间 DataFrame 保存到 OD 表中\n",
    "time_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(ods_table_name)\n",
    "\n",
    "# 从 OD 表中读取数据\n",
    "ods_time_df = spark.table(ods_table_name)\n",
    "\n",
    "# 显示 OD 表数据\n",
    "ods_time_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08990abe-ba3c-44f6-b70e-3686092190ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+-------+----+\n|      Date|      Day|  Month|Quarter|Year|\n+----------+---------+-------+-------+----+\n|2023-01-03|Wednesday|January|     Q1|2023|\n|2023-01-02|  Tuesday|January|     Q1|2023|\n|2023-01-01|   Monday|January|     Q1|2023|\n+----------+---------+-------+-------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# date 的dim表\n",
    "# 选择维度列\n",
    "ods_time_df = spark.table(\"exp1_ods_time_table\")\n",
    "\n",
    "# 选择维度列\n",
    "dimension_date_df = ods_time_df.select(\"Date\", \"Day\", \"Month\", \"Quarter\", \"Year\")\n",
    "\n",
    "# 如果有必要，添加额外的维度列\n",
    "# dimension_df = dimension_df.withColumn(\"SomeOtherDimension\", some_function())\n",
    "\n",
    "dim_table_name = \"exp1_dim_date_table\"\n",
    "\n",
    "# 写入维度表\n",
    "dimension_date_df.write.mode(\"overwrite\").saveAsTable(dim_table_name)\n",
    "\n",
    "dimension_date_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8cc643f-62ad-4698-94a6-b9171e84193e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+--------------------+------------+--------------------+\n|SupplierID|SupplierName|ContactPerson|             Address|       Phone|               Email|\n+----------+------------+-------------+--------------------+------------+--------------------+\n|        S2|  Supplier B|   Jane Smith|456 Oak St, City,...|456-789-0123|supplierB@example...|\n|        S1|  Supplier A|     John Doe|123 Main St, City...|123-456-7890|supplierA@example...|\n+----------+------------+-------------+--------------------+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# 定义供应商表结构\n",
    "supplier_schema = StructType([\n",
    "    StructField(\"SupplierID\", StringType(), True),\n",
    "    StructField(\"SupplierName\", StringType(), True),\n",
    "    StructField(\"ContactPerson\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"Phone\", StringType(), True),\n",
    "    StructField(\"Email\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 构造测试数据\n",
    "supplier_data = [\n",
    "    (\"S1\", \"Supplier A\", \"John Doe\", \"123 Main St, City, Country\", \"123-456-7890\", \"supplierA@example.com\"),\n",
    "    (\"S2\", \"Supplier B\", \"Jane Smith\", \"456 Oak St, City, Country\", \"456-789-0123\", \"supplierB@example.com\")\n",
    "]\n",
    "\n",
    "# 创建供应商 DataFrame\n",
    "supplier_df = spark.createDataFrame(supplier_data, schema=supplier_schema)\n",
    "\n",
    "# 加载到 ODS 表中\n",
    "# 假设 ODS 表已存在或需要创建\n",
    "ods_table_name = \"exp1_ods_supplier_table\"\n",
    "\n",
    "# 将时间 DataFrame 保存到 ODS 表中\n",
    "supplier_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(ods_table_name)\n",
    "\n",
    "# 从 ODS 表中读取数据\n",
    "suppliers_ods_df = spark.table(ods_table_name)\n",
    "\n",
    "# 展示 DataFrame\n",
    "suppliers_ods_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af941f79-3eb4-4401-9316-fbe6cebc4558",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+--------------------+------------+--------------------+\n|SupplierID|SupplierName|ContactPerson|             Address|       Phone|               Email|\n+----------+------------+-------------+--------------------+------------+--------------------+\n|        S2|  Supplier B|   Jane Smith|456 Oak St, City,...|456-789-0123|supplierB@example...|\n|        S1|  Supplier A|     John Doe|123 Main St, City...|123-456-7890|supplierA@example...|\n+----------+------------+-------------+--------------------+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 选择维度列\n",
    "ods_supplier_df = spark.table(\"exp1_ods_supplier_table\")\n",
    "\n",
    "# 选择维度列\n",
    "dimension_supplier_df = ods_supplier_df.select(\"SupplierID\", \"SupplierName\", \"ContactPerson\", \"Address\", \"Phone\", \"Email\")\n",
    "\n",
    "dim_table_name = \"exp1_dim_supplier_table\"\n",
    "\n",
    "# 写入维度表\n",
    "dimension_supplier_df.write.mode(\"overwrite\").saveAsTable(dim_table_name)\n",
    "\n",
    "dimension_supplier_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a3e2d2-4da7-4c61-9cdc-013188c0a262",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+--------+\n|WarehouseID|WarehouseName|   Location|Capacity|\n+-----------+-------------+-----------+--------+\n|         W2|  Warehouse B|Los Angeles|    1500|\n|         W1|  Warehouse A|   New York|    1000|\n|         W3|  Warehouse C|    Chicago|    1200|\n+-----------+-------------+-----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# 定义仓库表的结构\n",
    "warehouse_schema = StructType([\n",
    "    StructField(\"WarehouseID\", StringType(), True),\n",
    "    StructField(\"WarehouseName\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Capacity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# 构造测试数据\n",
    "test_data = [\n",
    "    (\"W1\", \"Warehouse A\", \"New York\", 1000),\n",
    "    (\"W2\", \"Warehouse B\", \"Los Angeles\", 1500),\n",
    "    (\"W3\", \"Warehouse C\", \"Chicago\", 1200)\n",
    "]\n",
    "\n",
    "# 将测试数据转换为 DataFrame\n",
    "test_df = spark.createDataFrame(test_data, schema=warehouse_schema)\n",
    "\n",
    "# 加载到 OD 表中\n",
    "# 假设 OD 表已存在或需要创建\n",
    "ods_table_name = \"exp1_ods_warehouse_table\"\n",
    "\n",
    "# 将时间 DataFrame 保存到 OD 表中\n",
    "test_df.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(ods_table_name)\n",
    "\n",
    "# 从 OD 表中读取数据\n",
    "warehouse_ods_df = spark.table(ods_table_name)\n",
    "\n",
    "# 展示 DataFrame\n",
    "warehouse_ods_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cffa3d7-c704-4f10-b607-e71d37003e85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+--------+\n|WarehouseID|WarehouseName|   Location|Capacity|\n+-----------+-------------+-----------+--------+\n|         W2|  Warehouse B|Los Angeles|    1500|\n|         W1|  Warehouse A|   New York|    1000|\n|         W3|  Warehouse C|    Chicago|    1200|\n+-----------+-------------+-----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 选择维度列\n",
    "ods_warehouse_df = spark.table(\"exp1_ods_warehouse_table\")\n",
    "\n",
    "# 选择维度列\n",
    "dimension_warehouse_df = ods_warehouse_df.select(\"WarehouseID\", \"WarehouseName\", \"Location\", \"Capacity\")\n",
    "\n",
    "dim_table_name = \"exp1_dim_warehouse_table\"\n",
    "\n",
    "# 写入维度表\n",
    "dimension_warehouse_df.write.mode(\"overwrite\").saveAsTable(dim_table_name)\n",
    "\n",
    "dimension_warehouse_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e09baac0-f034-4994-b30e-2dfe7eba1b38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+----------+-----------+---------+------+------------+-------------+-----------------+\n|ProductID|WarehouseID|Quantity| StockDate|ProductName| Category| Brand|StockQuarter|WarehouseName|WarehouseLocation|\n+---------+-----------+--------+----------+-----------+---------+------+------------+-------------+-----------------+\n|       P2|         W2|     150|2023-01-02|   Product2|Category2|Brand2|          Q1|  Warehouse B|      Los Angeles|\n|       P1|         W1|     100|2023-01-01|   Product1|Category1|Brand1|          Q1|  Warehouse A|         New York|\n|       P3|         W1|     200|2023-01-03|   Product3|Category3|Brand3|          Q1|  Warehouse A|         New York|\n+---------+-----------+--------+----------+-----------+---------+------+------------+-------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 选择维度列\n",
    "ods_inventory_df = spark.table(\"exp1_ods_inventory_table\")\n",
    "dim_product_df = spark.table(\"exp1_dim_product_table\")\n",
    "dim_date_df = spark.table(\"exp1_dim_date_table\")\n",
    "dim_warehouse_df = spark.table(\"exp1_dim_warehouse_table\")\n",
    "\n",
    "# 选择dwd表的列\n",
    "dwd_inventory_df = ods_inventory_df.select(\"Product ID\", \"Warehouse ID\", \"Quantity\", \"Stock Date\")\n",
    "\n",
    "dwd_inventory_df = dwd_inventory_df\\\n",
    "                    .join(dim_product_df, dwd_inventory_df['Product ID'] == dim_product_df['Product ID'],\"left\")\\\n",
    "                    .select(dwd_inventory_df[\"Product ID\"],\n",
    "                            dwd_inventory_df[\"Warehouse ID\"],\n",
    "                            dwd_inventory_df[\"Quantity\"],\n",
    "                            dwd_inventory_df[\"Stock Date\"],\n",
    "                            dim_product_df['Product Name'],\n",
    "                            dim_product_df['Category'],\n",
    "                            dim_product_df['Brand'])\n",
    "dwd_inventory_df = dwd_inventory_df\\\n",
    "                    .join(dim_date_df, dwd_inventory_df['Stock Date'] == dim_date_df['Date'],\"left\")\\\n",
    "                    .select(dwd_inventory_df[\"Product ID\"],\n",
    "                            dwd_inventory_df[\"Warehouse ID\"],\n",
    "                            dwd_inventory_df[\"Quantity\"],\n",
    "                            dwd_inventory_df[\"Stock Date\"],\n",
    "                            dwd_inventory_df['Product Name'],\n",
    "                            dwd_inventory_df['Category'],\n",
    "                            dwd_inventory_df['Brand'],\n",
    "                            dim_date_df['Quarter'].alias(\"Stock Quarter\"))\n",
    "dwd_inventory_df = dwd_inventory_df\\\n",
    "                    .join(dim_warehouse_df, dwd_inventory_df['Warehouse ID'] == dim_warehouse_df['WarehouseID'],\"left\")\\\n",
    "                    .select(dwd_inventory_df[\"Product ID\"].alias(\"ProductID\"),\n",
    "                            dwd_inventory_df[\"Warehouse ID\"].alias(\"WarehouseID\"),\n",
    "                            dwd_inventory_df[\"Quantity\"],\n",
    "                            dwd_inventory_df[\"Stock Date\"].alias(\"StockDate\"),\n",
    "                            dwd_inventory_df['Product Name'].alias(\"ProductName\"),\n",
    "                            dwd_inventory_df['Category'],\n",
    "                            dwd_inventory_df['Brand'],\n",
    "                            dwd_inventory_df['Stock Quarter'].alias(\"StockQuarter\"),\n",
    "                            dim_warehouse_df['WarehouseName'],\n",
    "                            dim_warehouse_df['Location'].alias(\"WarehouseLocation\"))\\\n",
    "\n",
    "dwd_table_name = \"exp1_dwd_inventory_table\"\n",
    "\n",
    "# 写入维度表\n",
    "dwd_inventory_df.write.mode(\"overwrite\").saveAsTable(dwd_table_name)\n",
    "\n",
    "dwd_inventory_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0744eecd-4f18-4078-b075-3e9f0a4c1b95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+--------+----------+---------+----------+--------------+-----------+---------+------+\n|OrderID|ProductID|CustomerID|Quantity| SalesDate|UnitPrice|TotalPrice| PaymentMethod|ProductName| Category| Brand|\n+-------+---------+----------+--------+----------+---------+----------+--------------+-----------+---------+------+\n| ORD003|       P3|   CUST003|       1|2023-01-03|     20.0|      20.0|Online Payment|   Product3|Category3|Brand3|\n| ORD002|       P2|   CUST002|       3|2023-01-02|     15.0|      45.0|   Credit Card|   Product2|Category2|Brand2|\n| ORD001|       P1|   CUST001|       2|2023-01-01|     10.0|      20.0|          Cash|   Product1|Category1|Brand1|\n+-------+---------+----------+--------+----------+---------+----------+--------------+-----------+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 选择维度列\n",
    "ods_sales_df = spark.table(\"exp1_ods_sales_table\")\n",
    "dim_product_df = spark.table(\"exp1_dim_product_table\")\n",
    "\n",
    "# 选择dwd表的列\n",
    "dwd_sales_df = ods_sales_df.select(\"Order ID\", \"Product ID\", \"Customer ID\", \"Quantity\",\"Sales Date\",\"Unit Price\",\"Total Price\",\"Payment Method\")\n",
    "\n",
    "dwd_sales_df = dwd_sales_df\\\n",
    "                    .join(dim_product_df, dwd_sales_df['Product ID'] == dim_product_df['Product ID'],\"left\")\\\n",
    "                    .select(dwd_sales_df[\"Order ID\"].alias(\"OrderID\"),\n",
    "                            dwd_sales_df[\"Product ID\"].alias(\"ProductID\"),\n",
    "                            dwd_sales_df[\"Customer ID\"].alias(\"CustomerID\"),\n",
    "                            dwd_sales_df[\"Quantity\"],\n",
    "                            dwd_sales_df[\"Sales Date\"].alias(\"SalesDate\"),\n",
    "                            dwd_sales_df['Unit Price'].alias(\"UnitPrice\"),\n",
    "                            dwd_sales_df['Total Price'].alias(\"TotalPrice\"),\n",
    "                            dwd_sales_df['Payment Method'].alias(\"PaymentMethod\"),\n",
    "                            dim_product_df['Product Name'].alias(\"ProductName\"),\n",
    "                            dim_product_df['Category'],\n",
    "                            dim_product_df['Brand'])\n",
    "\n",
    "dwd_table_name = \"exp1_dwd_sales_table\"\n",
    "# 写入维度表\n",
    "dwd_sales_df.write.mode(\"overwrite\").saveAsTable(dwd_table_name)\n",
    "\n",
    "dwd_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f55ea86-8bfe-4482-a07c-3119f1bb0cd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 停止 SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dw-exp1-inventory-analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
